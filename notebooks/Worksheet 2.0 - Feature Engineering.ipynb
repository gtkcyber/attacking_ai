{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 2.0: Feature Engineering\n",
    "This worksheet covers concepts covered in the first part of the Feature Engineering module.  It should take no more than 30-40 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "This worksheet is a step-by-step guide on how to detect domains that were generated using \"Domain Generation Algorithm\" (DGA). We will walk you through the process of transforming raw domain strings to Machine Learning features and creating a decision tree classifer which you will use to determine whether a given domain is legit or not. Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  \n",
    "\n",
    "Overview 2 main steps:\n",
    "\n",
    "1. **Feature Engineering** - from raw domain strings to numeric Machine Learning features using DataFrame manipulations\n",
    "2. **Machine Learning Classification** - predict whether a domain is legit or not using a Decision Tree Classifier\n",
    "  \n",
    "\n",
    "**DGA - Background**\n",
    "\n",
    "\"Various families of malware use domain generation\n",
    "algorithms (DGAs) to generate a large number of pseudo-random\n",
    "domain names to connect to a command and control (C2) server.\n",
    "In order to block DGA C2 traffic, security organizations must\n",
    "first discover the algorithm by reverse engineering malware\n",
    "samples, then generate a list of domains for a given seed. The\n",
    "domains are then either preregistered, sink-holed or published\n",
    "in a DNS blacklist. This process is not only tedious, but can\n",
    "be readily circumvented by malware authors. An alternative\n",
    "approach to stop malware from using DGAs is to intercept DNS\n",
    "queries on a network and predict whether domains are DGA\n",
    "generated. Much of the previous work in DGA detection is based\n",
    "on finding groupings of like domains and using their statistical\n",
    "properties to determine if they are DGA generated. However,\n",
    "these techniques are run over large time windows and cannot be\n",
    "used for real-time detection and prevention. In addition, many of\n",
    "these techniques also use contextual information such as passive\n",
    "DNS and aggregations of all NXDomains throughout a network.\n",
    "Such requirements are not only costly to integrate, they may not\n",
    "be possible due to real-world constraints of many systems (such\n",
    "as endpoint detection). An alternative to these systems is a much\n",
    "harder problem: detect DGA generation on a per domain basis\n",
    "with no information except for the domain name. Previous work\n",
    "to solve this harder problem exhibits poor performance and many\n",
    "of these systems rely heavily on manual creation of features;\n",
    "a time consuming process that can easily be circumvented by\n",
    "malware authors...\"    \n",
    "[Citation: Woodbridge et. al 2016: \"Predicting Domain Generation Algorithms with Long Short-Term Memory Networks\"]\n",
    "\n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (https://matplotlib.org/)\n",
    "* Seaborn (https://seaborn.pydata.org)\n",
    "* YData Profiling: (https://ydata-profiling.ydata.ai/docs/master/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T19:28:25.038396Z",
     "start_time": "2025-09-10T19:28:25.036757Z"
    }
   },
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "DATA_HOME = '../data/'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "full_df = pd.read_csv('../data/dga_data_small.csv')\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:29.971759Z",
     "start_time": "2023-08-01T14:28:29.940821Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df.drop(['host', 'subclass'], axis='columns', inplace=True)\n",
    "print(\"(Total Rows, Total Cols)\")\n",
    "print(full_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print a random sample of the DataFrame\n",
    "full_df.sample(5).head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:30.764119Z",
     "start_time": "2023-08-01T14:28:30.748261Z"
    }
   },
   "outputs": [],
   "source": [
    "# print a sample of dga domains via filtering\n",
    "full_df[full_df.isDGA == 'dga'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Feature Engineering\n",
    "\n",
    "Option 1 to derive Machine Learning features is to manually hand-craft useful contextual information of the domain string. \n",
    "\n",
    "Previous academic research has focused on the following features that are based on contextual information:\n",
    "\n",
    "**List of features**:\n",
    "\n",
    "1. Length [\"length\"]\n",
    "2. Number of digits [\"digits\"]\n",
    "3. Entropy [\"entropy\"] - use ```H_entropy``` function provided \n",
    "4. Vowel to consonant ratio [\"vowel-cons\"] - use ```vowel_consonant_ratio``` function provided\n",
    "5. The index of the first digit - use the ``firstDigitIndex`` function provided\n",
    "5. N-grams [\"n-grams\"] - use ```ngram``` functions provided\n",
    "\n",
    "**Tasks**:    \n",
    "Split into A and B parts, see below...\n",
    "\n",
    "Please run the following function cell and then continue reading the next markdown cell with more details on how to derive those features. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:33.965591Z",
     "start_time": "2023-08-01T14:28:33.956856Z"
    }
   },
   "outputs": [],
   "source": [
    "def H_entropy (x):\n",
    "    # Calculate Shannon Entropy\n",
    "    prob = [ float(x.count(c)) / len(x) for c in dict.fromkeys(list(x)) ] \n",
    "    H = - sum([ p * np.log2(p) for p in prob ]) \n",
    "    return H\n",
    "\n",
    "def firstDigitIndex(s):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isdigit():\n",
    "            return i + 1\n",
    "    return 0\n",
    "\n",
    "def vowel_consonant_ratio (x):\n",
    "    # Calculate vowel to consonant ratio\n",
    "    x = x.lower()\n",
    "    vowels_pattern = re.compile('([aeiou])')\n",
    "    consonants_pattern = re.compile('([b-df-hj-np-tv-z])')\n",
    "    vowels = re.findall(vowels_pattern, x)\n",
    "    consonants = re.findall(consonants_pattern, x)\n",
    "    try:\n",
    "        ratio = len(vowels) / len(consonants)\n",
    "    except: # catch zero devision exception \n",
    "        ratio = 0  \n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task A: Derive new features\n",
    "\n",
    "Please try to derive a new pandas 2D DataFrame with a new column for each of feature. Focus on \n",
    "1. Length [\"length\"]\n",
    "2. Number of digits [\"digits\"]\n",
    "3. Entropy [\"entropy\"] - use ```H_entropy``` function provided \n",
    "4. Vowel to consonant ratio [\"vowel-cons\"] - use ```vowel_consonant_ratio``` function provided\n",
    "5. The index of the first digit - use the ``firstDigitIndex`` function provided\n",
    "\n",
    "[pandas.Series.str](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html#pandas.Series.str), [pandas.Series.replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.replace.html) and [pandas.Series,apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html) can be very helpful to quickly derive those features. Functions you need to apply here are provided in above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:35.574595Z",
     "start_time": "2023-08-01T14:28:35.350703Z"
    }
   },
   "outputs": [],
   "source": [
    "# derive features\n",
    "full_df['length'] = full_df['domain'].str.len()\n",
    "full_df['digits'] = #Your Code\n",
    "full_df['entropy'] = #Your Code\n",
    "full_df['vowel-cons'] = #Your Code\n",
    "full_df['firstDigitIndex'] = #Your Code\n",
    "\n",
    "print(full_df['isDGA'].value_counts())\n",
    "\n",
    "# check intermediate 2D pandas DataFrame by printing a few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task B - Ngram featrue\n",
    "\n",
    "Finally, let's tackle the **ngram** feature. There are multiple steps involved to derive this feature. Here in this notebook, we use an implementation outlined in the this academic paper [Schiavoni 2014: \"Phoenix: DGA-based Botnet Tracking and Intelligence\" - see section: Linguistic Features](https://seclab.cs.ucsb.edu/files/publications/Schiavoni2014Phoenix_DGA-Based.pdf).\n",
    "\n",
    "\n",
    "- **What are ngrams???** Imagine a string like 'facebook', if I were to derive all n-grams for n=2 (aka bi-grams) I would get '['fa', 'ac', 'ce', 'eb', 'bo', 'oo', 'ok']', so you see that you slide with one step from the left and just group 2 characters together each time, a tri-gram for 'facebook' would yielfd '['fac', 'ace', 'ceb', 'ebo', 'boo', 'ook']'. Ngrams have a long history in natural language processing, but are also used a lot for example in detecting malicious executable (raw byte ngrams in this case).\n",
    "\n",
    "Steps involved:\n",
    "\n",
    "1. We have the 10000 most common english words (see data file we loaded, we call this DataFrame ```top_en_words```). Now we run the ```ngrams``` functions on a list of all these words. The output here is a list that contains ALL 1-grams, bi-grams and tri-grams of these 10000 most common english words.\n",
    "2. We used the ```Counter``` function from collections to derive a dictionary ```ngram_common_dict``` that contains the counts of all unique 1-grams, bi-grams and tri-grams.\n",
    "3. Our ```ngram_feature``` function will do the core magic. It takes your domain as input, splits it into ngrams (n is a function parameter) and then looks up these ngrams in the english dictionary ```ngram_common_dict```. Function returns the normalized sum of all ngrams that were contained in the english dictionary. For example, running ```ngram_feature('facebook', d, 2)``` will return 171.28 (this value is just like the one published in the Schiavoni paper).\n",
    "4. Finally ```average_ngram_feature``` wraps around ```ngram_feature```. All it does is take the average of a list of numbers b/c your task is to derive a feature that gives the average of the ngram_feature for three different values of ```n``` n=1,2,3.    \n",
    "5. **YOUR TURN: Apply ```average_ngram_feature``` to you domain column in the DataFrame thereby adding ```ngram``` to the full_df.**\n",
    "6. **YOUR TURN: Finally drop the ```domain``` column from your DataFrame**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "We have the 10000 most common english words ```top_en_words```.  Load this .txt file using the Pandas read_csv function and print 5 random rows of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Google's 10000 most common english words will be needed to derive a feature called ngrams\n",
    "\n",
    "top_en_words = pd.read_csv('../data/google-10000-english.txt', header=None, names=['words'])\n",
    "\n",
    "# print 5 rows\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "We used the ```Counter``` function from collections to derive a dictionary that contains the counts of all unique 1-grams, bi-grams and tri-grams in the file you loaded above [common english source](https://github.com/first20hours/google-10000-english). \n",
    "\n",
    "This is saved as ```d_common_en_words.pickle``` in the ```/data``` directory. \n",
    "\n",
    "Load this dictionary into a variable called ```ngram_common_dict```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file is located at ../data/d_common_en_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Define functions to calculate the average ngram feature for a given word (domain). We have created the functions below, you just need to run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:36.840195Z",
     "start_time": "2023-08-01T14:28:36.826032Z"
    }
   },
   "outputs": [],
   "source": [
    "# Schiavoni 2014: \"Phoenix: DGA-based Botnet Tracking and Intelligence\" - see section: Linguistic Features\n",
    "# https://seclab.cs.ucsb.edu/files/publications/Schiavoni2014Phoenix_DGA-Based.pdf\n",
    "\n",
    "def ngrams(word, n):\n",
    "    '''\n",
    "    Extract n ngrams from each word and return a regular Python list\n",
    "    \n",
    "    Input \n",
    "    word: (string) or a (list) of strings\n",
    "    n: (integer) or a (list) of integers, lenght of ngram\n",
    "\n",
    "    Output: \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    list_ngrams = []\n",
    "    if isinstance(word, list):\n",
    "        for w in word:\n",
    "            if isinstance(n, list):\n",
    "                for curr_n in n:\n",
    "                    ngrams = [w[i:i+curr_n] for i in range(0,len(w)-curr_n+1)]\n",
    "                    list_ngrams.extend(ngrams)\n",
    "            else:\n",
    "                ngrams = [w[i:i+n] for i in range(0,len(w)-n+1)]\n",
    "                list_ngrams.extend(ngrams)\n",
    "    else:\n",
    "        if isinstance(n, list):\n",
    "            for curr_n in n:\n",
    "                ngrams = [word[i:i+curr_n] for i in range(0,len(word)-curr_n+1)]\n",
    "                list_ngrams.extend(ngrams)\n",
    "        else:\n",
    "            ngrams = [word[i:i+n] for i in range(0,len(word)-n+1)]\n",
    "            list_ngrams.extend(ngrams)\n",
    "\n",
    "    return list_ngrams\n",
    "\n",
    "def ngram_feature(word, common_dict, n):\n",
    "    '''\n",
    "    Takes (word) as input, splits it into (n) ngrams and then looks up and counts where\n",
    "    these ngrams are found in the (common_dict). Function returns the normalized sum of all \n",
    "    ngrams that were found in the (common_dict). \n",
    "    \n",
    "    For example, ngram_feature('facebook', ngram_common_dict, 2) will return 171.28\n",
    "    \n",
    "    Input \n",
    "    word: (str) or (list) of strings (domain in our case)\n",
    "    common_dict: (dictionary) that contains the count for most common english words\n",
    "    n: (int) or (list) of the ngram length example: 1,2,3\n",
    "    \n",
    "    Output\n",
    "    feature: (float) a normalized sum of ngram count found in the common_dict\n",
    "    '''\n",
    "\n",
    "    # get a list of matching ngrams from common dict. \n",
    "    list_ngrams = ngrams(word, n)\n",
    "    \n",
    "    count_sum=0\n",
    "    \n",
    "    for ngram in list_ngrams:\n",
    "        if common_dict[ngram]:\n",
    "            count_sum += common_dict[ngram]\n",
    "    try:\n",
    "        feature = count_sum/(len(word)-n+1)\n",
    "    except:\n",
    "        feature = 0\n",
    "    return feature\n",
    "    \n",
    "def average_ngram_feature(word):\n",
    "    \n",
    "    '''\n",
    "    Takes a word (word) as input, uses the ngram_feature and ngrms functions (above) to create ngrams, \n",
    "    get the sum of the ngram count found in the common_dict for 1, 2 and 3-grams. \n",
    "    Then, Calculate the average of these three results.\n",
    "    \n",
    "    Input: \n",
    "    Output: average of the list \n",
    "    '''\n",
    "    ngram_counts = []\n",
    "    num_of_grams = [1,2,3]\n",
    "    for n in num_of_grams:\n",
    "        ngram_counts.append(ngram_feature(word, ngram_common_dict, n))\n",
    "                  \n",
    "    return sum(ngram_counts)/len(ngram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 & 5\n",
    "Get the ```average_ngram_feature``` calculated for each row (domain) and store it in a new column called **ngrams**.\n",
    "\n",
    "Hint: You can put it all together in one line of code with the Pandas **apply** method to vectorize this operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:37.718471Z",
     "start_time": "2023-08-01T14:28:37.658088Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_df['ngrams'] = # YOUR CODE HERE\n",
    "\n",
    "# check final 2D pandas DataFrame containing all final features and the target vector isDGA\n",
    "full_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "1. Save this full dataset as a csv\n",
    "2. Drop the **target** column and save that dataframe as a csv as a **feature** dataset that is ready for modeling. \n",
    "\n",
    "This code is already written, you only need to run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:38.357481Z",
     "start_time": "2023-08-01T14:28:38.309191Z"
    }
   },
   "outputs": [],
   "source": [
    "full_df.to_csv('../data/dga_features_final_df_domain.csv', index=False)\n",
    "\n",
    "df_final = full_df.drop(['domain'], axis='columns')\n",
    "df_final.to_csv('../data/dga_features_final_df.csv', index=False)\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakpoint: Load Features and Labels\n",
    "\n",
    "If you got stuck in Part 1, please simply uncomment the code here and load the feature matrix we prepared for you, so you can move on to visualizing your new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:39.688903Z",
     "start_time": "2023-08-01T14:28:39.664406Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_final = pd.read_csv('../data/our_data_dga_features_final_df.csv')\n",
    "#print(df_final['isDGA'].value_counts())\n",
    "#df_final.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Visualizing the Results\n",
    "At this point, we've created a dataset which has many features that can be used for classification.  Your final step is to visualize the features is likely to be of value. That is, which features are likely to help our model distinguish between our two classes of **dga** and **legit**. \n",
    "\n",
    "## Correlation of Features with Targets\n",
    "First we will compute a correlation matrix to see how correlated each feature is with the target variable. This can be a indicator of how predictive a feature will be and also show us a little bit about how features might be related. Then we will look at how the features are correlated with each other. \n",
    "\n",
    "### Step 1: Separate features and target\n",
    "This is a step you will do many many more times in your ML adventures. You need to create a dataframe that contains only the feature columns, and another that contains only the target column **isDGA**. \n",
    "\n",
    "Create 2 new dataframes that satisfy this requirement called ```features``` and ```target```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:28:40.793540Z",
     "start_time": "2023-08-01T14:28:40.789532Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names = # create a list of the column names of the features\n",
    "features = # Create a dataframe that contains only the feature columns\n",
    "target = # Create a dataframe that contains only the target columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Encode Targets\n",
    "Many functions (and models) require numbers as input for the targets, rather than strings. Create another dataframe of our targets that contains only numbers:\n",
    "- 0 = legit\n",
    "- 1 = dga\n",
    "\n",
    "There are many ways to accomplish this task! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the strings in the target column with numbers\n",
    "df_final_int = #YOUR CODE HERE \n",
    "df_final_int.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create the Correlation\n",
    "Use Pandas ```.corr``` method to calculate the correlation of each feature to the target variable. \n",
    "\n",
    "This code is written, just run the cell below. Note that we are sorting the values afterwards to see them in order when we plot. Try commenting this out to see what it changes in the plot. \n",
    "\n",
    "What shape is the output of this calculation? Is this what you expected?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_results = df_final_int.corr()['isDGA'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Profit\n",
    "Plot the results of this calculation as a bar graph. You can use the built-in pandas function ```.plot``` (easiest) or the matplotlib or seaborn (most attractive) libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE, plot the corr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation of Features\n",
    "Now we want to know how the features are correlated with each other (as well as the target). A good way to visualize this is with a heatmap of the correlation matrix. \n",
    "\n",
    "1. Compute the correlations between all the features. (Code is there, just run the cell to use the Pandas ```.corr``` method on the full matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df_final_int.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "2. use the seaborn library (it is a wrapper for matplotlib) to plot these results as a heatmap.\n",
    "\n",
    "The code is already here, but take a look at using different cmap to get different colorways. [cmap ref](https://matplotlib.org/stable/gallery/color/colormap_reference.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a mask for the upper triangle (b/c it contains the same information as the lower one. \n",
    "mask = np.triu(np.ones_like(corr_mat, dtype=bool))\n",
    "\n",
    "# plot a heatmap\n",
    "sns.heatmap(corr_mat, mask=mask, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairplot\n",
    "Now let's use a Seaborn pairplot as well.  This will really show you which features have clear dividing lines between the classes.  Docs are available here: http://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "\n",
    "When we pass the full matrix to seaborn (notice here we want to be using our string-based target variable), we need to tell it which column we want to use to color the plots. Using the target variable is key becuase it  helps us visualize the patterns according to our targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:29:20.996229Z",
     "start_time": "2023-08-01T14:28:55.522600Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_final, hue='isDGA');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pairplots can be a little confusing. \n",
    "\n",
    "First, take a look at the diagonal. We see the density of each feature against our 2 target classes. If we can see that there is little overlap, then it's generally an indicator that this feature would be a good predictor of the target and is a good candidate to include in the model. \n",
    "\n",
    "Second, let's look at these scatter plots. You can ignor either the top or bottom diagonal b/c it's the same information. Now what we want to know is whether any of these features are contributing the same **information** about our variable. The idea is that if 2 features are highly correlated, then it's likely they contribute the same information and one could (possibly should) be removed from our model. Two correlated features will look like a diagonal line (pos or neg). \n",
    "\n",
    "Which feature(s) look like they are contributing similar information? \n",
    "\n",
    "Which features look like they would be good predictors? \n",
    "\n",
    "## Radviz\n",
    "Finally, let's try making a RadViz (2D projection) of the features using the Pandas method ```.radviz```. \n",
    "\n",
    "Hoffman, P. E. et al. (1997) DNA visual and analytic data mining. In the Proceedings of the IEEE Visualization. Phoenix, AZ, pp. 437-441\n",
    "\n",
    "Each Series in the DataFrame is represented as a evenly distributed slice on a circle. Each data point is rendered in the circle according to the value on each Series. Highly correlated Series in the DataFrame are placed closer on the unit circle.\n",
    "\n",
    "RadViz allow to project a N-dimensional data set into a 2D space where the influence of each dimension can be interpreted as a balance between the influence of all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.radviz(df_final, 'isDGA', colormap='cividis');#colormap='Set3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Profiling\n",
    "You've seen how to do all these steps manually. In our final example for data exploration and feature engineering, you will use a new module called Pandas Profiling to explore your data.  This module will generate much of the same information but it is a major time saver in that it will do all these calculations for you in one command.  \n",
    "\n",
    "Basic usage is:\n",
    "\n",
    "```python\n",
    "profile = ProfileReport(df_final, title='Pandas Profiling Report')\n",
    "profile\n",
    "```\n",
    "\n",
    "In the cell below, execute this command and take a look at the report which is generated to see if you can find any insights about the features we've generated.  After looking at all this, do you think these features will be good enough?\n",
    "\n",
    "\n",
    "Documentation is available here: https://pandas-profiling.github.io/pandas-profiling/docs/master/rtd/pages/getting_started.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you are getting errors running the cell below, try one of these commands to get the versioning of the libraries correct for ydata-profiling to work\n",
    "#!pip install ydata-profiling\n",
    "#!pip install --upgrade ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-01T14:29:47.366144Z",
     "start_time": "2023-08-01T14:29:32.234170Z"
    }
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = #YOUR CODE HERE\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Congrats!\n",
    "Congrats!  You've now extracted features from the dataset and are ready to begin creating some supervised models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
