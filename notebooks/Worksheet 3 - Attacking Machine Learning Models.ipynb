{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3bf6ec",
   "metadata": {},
   "source": [
    "<img src=\"../img/GTK_Logo_Social Icon.jpg\" width=175 align=\"right\" />\n",
    "\n",
    "# Worksheet 3 Attacking Machine Learning Models\n",
    "\n",
    "In this lab, we will learn how to use the Adversarial Robustness Toolkit (ART) to launch various attacks against models. The first attack you will launch will be to create adversarial examples from a model.  These examples could be used to defeat a model, or control the model's behavior.\n",
    "\n",
    "The documentation for ART can be found here: https://github.com/Trusted-AI/adversarial-robustness-toolbox/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "id": "2cb78309",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from art.attacks.evasion import DecisionTreeAttack, HopSkipJump\n",
    "from art.estimators.classification import SklearnClassifier, BlackBoxClassifier\n",
    "from art.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "DATA_HOME = '../data'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d975a6d",
   "metadata": {},
   "source": [
    "## Decision Tree Attack\n",
    "In this example, we are going to use the ART to attack a decision tree. The goal is to create adversarial examples which could be used to control the output of the model.  \n",
    "\n",
    "Due to the nature of decision trees, it is not necessary to use gradient descent to discover adversarial examples and instead, it can be accomplished by tree traversals. This attack is a whitebox attack in that you need to have access to the actual model. \n",
    "\n",
    "This methodology was described in a paper by Papernot et al. in https://arxiv.org/abs/1605.07277. You can see this code in action here: https://github.com/Trusted-AI/adversarial-robustness-toolbox/blob/main/notebooks/attack_decision_tree.ipynb.\n",
    "\n",
    "First we're going to load the model from a pickle file. "
   ]
  },
  {
   "cell_type": "code",
   "id": "68db8841",
   "metadata": {},
   "source": [
    "# Load the classifier from the pickle file\n",
    "with open(f\"{DATA_HOME}/dga_decision_tree_adv.pkl\", \"rb\") as file:\n",
    "    clf = joblib.load(file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed2e8c6de3aabdb5",
   "metadata": {},
   "source": [
    "clf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb8243ae",
   "metadata": {},
   "source": [
    "We will also need some training data.  In this case, we'll use the data that was used to train the original model, but this is not necessary. "
   ]
  },
  {
   "cell_type": "code",
   "id": "e3d41937",
   "metadata": {},
   "source": [
    "df = pd.read_csv(f'{DATA_HOME}/dga_features_final_df.csv')\n",
    "label_encoder = LabelEncoder()\n",
    "target = label_encoder.fit_transform(df['isDGA'])\n",
    "\n",
    "feature_matrix = df.drop(['isDGA'], axis=1)\n",
    "feature_matrix_train, feature_matrix_test, target_train, target_test = train_test_split(feature_matrix, target, test_size=0.25)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98c4a605",
   "metadata": {},
   "source": [
    "### Step 1:  Create the ART Classifier\n",
    "As a first step, we need to use ART to create an \"adversarial\" classifier.  Use the `SklearnClassifier` module from ART. "
   ]
  },
  {
   "cell_type": "code",
   "id": "7446f161",
   "metadata": {},
   "source": [
    "# Your code here ...\n",
    "adversarial_classifier ="
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ef62f2c",
   "metadata": {},
   "source": [
    "### Step 2:  Attack!!!\n",
    "Now that you've created an adversarial classifier the next step is to train that adversarial classifier.  Use the `DecisionTreeAttack` module in ART to launch an attack, then call the `generate()` with the `feature_matrix_train` and `target_train` datasets.  The `generate()` method can be called either with only the feature matrix alone or you can call it with a list of desired targets.  \n",
    "\n",
    "For our example, let's say that we want all the results to be classified as legitimate, we're going to pass it a numpy array of 1500 `0` for a target vector.\n",
    "\n",
    "\n",
    "Note: You will have to call the `.to_numpy()` methods on these datasets when you pass them to ART.\n",
    "\n",
    "\n",
    "This step generates a lot of future warnings. For this exercise we have suppressed them, however scikit-learn will throw warnings when you mix numpy arrays and dataframes.  The way to avoid this is to actually train your models on numpy arrays.  To do that, during the training process, convert the dataframe to a numpy array with the `.to_numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "id": "3824ca45",
   "metadata": {},
   "source": [
    "# Here's an array of all zeros to fool the classifier\n",
    "all_legit = np.array([0] * 1500)\n",
    "\n",
    "# First create the DecisionTreeAttack\n",
    "attack = # Your code here...\n",
    "\n",
    "# Then run the generate function to generate adversarial examples.\n",
    "adversarial_data = # Your code here..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "adversarial_preds = # Your code here...",
   "id": "2343144d1fc32d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77617eb9",
   "metadata": {},
   "source": [
    "### Step 3:  Evaluate the Performance\n",
    "At this point you should have a dataset of adversarial examples that produce exclusively legit classifications.  Now try running that through the original classifier and making a confusion matrix and classification report to see how we did."
   ]
  },
  {
   "cell_type": "code",
   "id": "2b8adf34",
   "metadata": {},
   "source": "# Your code here...",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T03:28:16.122366Z",
     "start_time": "2025-09-11T03:28:16.120181Z"
    }
   },
   "cell_type": "code",
   "source": "# Your code here...",
   "id": "6a90c6969f9658c8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "c9c979d7",
   "metadata": {},
   "source": [
    "If you did this correctly, you should get predictions that are entirely of the `0` class.  This shows how you are able to generate adverarial data that can be crafted to direct the decisions of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce24cb02",
   "metadata": {},
   "source": [
    "## BlackBox Adversarial Attack\n",
    "Now that you've successfully launched a white box adversarial attack, let's try a blackbox attack. We're going to use the `HopSkipJump` attack from Jianbo et al. (2019). This is a powerful black-box attack that only requires final class prediction, and is an advanced version of the boundary attack.\n",
    "\n",
    "Paper link: https://arxiv.org/abs/1904.02144\n",
    "\n",
    "In order to execute this attack, we will need a `predict()` function which calls a trained model and returns the predictions. In our example, the `predict()` function is simply a wrapper for our trained classifier, however, this same technique could be used with a true blackbox model where only the predictions are accessible. In that case, the `predict()` function would contain API calls or something similar."
   ]
  },
  {
   "cell_type": "code",
   "id": "2d5848c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-11T03:28:31.524142Z",
     "start_time": "2025-09-11T03:28:31.521099Z"
    }
   },
   "source": [
    "def predict(x):\n",
    "    '''\n",
    "    Call the model and return the predictions.  This function could contain calls to a true \n",
    "    blackbox model, but in this example, is calling our pre-trained model.\n",
    "    '''\n",
    "    x = np.array(x)\n",
    "    return to_categorical(clf.predict(x), nb_classes=2)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "c181d362",
   "metadata": {},
   "source": [
    "### Step 1:  Create the BlackBox Classifier\n",
    "In order to execute the attack we need to first create a `BlackBoxClassifier`.  At a minimum, we need to pass the predict function, the number of features and the number of possible classes."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9509e5b",
   "metadata": {},
   "source": "blackbox_clf = # Your code here...",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15a96463",
   "metadata": {},
   "source": [
    "### Step 2:  ATTACK!!  Generate Adversarial Examples\n",
    "The next step is to create the `HopSkipJump` object to launch the attack.  This follows a similar pattern as the previous attack where you create the `attack` object, then call the `generate()` method passing the testing features (`feature_matrix_test`).  This will generate an array of adversarial examples.  \n",
    "\n",
    "For our use case, let's say that we want to generate adversarial examples that skew towards one class. In the `HopSkipJump` object, set `targeted=True` which forces the attack to generate examples for one class only. \n",
    "\n",
    "\n",
    "NOTE: You will have to convert the testing features to a numpy array like this:\n",
    "```python\n",
    "feature_matrix_test.to_numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ea84d71",
   "metadata": {},
   "source": [
    "# Create the attack object.\n",
    "attack =\n",
    "adversarial_data_blackbox =\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1d8fd09",
   "metadata": {},
   "source": [
    "### Step 3:  Evaluate the Attack\n",
    "Now that you have a set of adversarial data, let's make some predictions with that data and see how effective it is in predicting the model output.  You won't be able to use Yellowbrick here because the `BlackBoxClassifier` does not implement the `fit()` method. \n",
    "\n",
    "For this final step, make the predictions, then create a confusion matrix of this data to evaluate your BlackBox model's performance."
   ]
  },
  {
   "cell_type": "code",
   "id": "80f4cf47",
   "metadata": {},
   "source": [
    "# First make some predictions using the adversarial data you generated\n",
    "adversarial_predictions = # Your code here..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e79fe0d",
   "metadata": {},
   "source": [
    "# Now create a confusion matrix\n",
    "confusion_matrix= metrics.confusion_matrix(target_test, adversarial_predictions)\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix).plot()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "115cdb56",
   "metadata": {},
   "source": [
    "How did the model do?  If you did this correctly, you should have a blackbox classifier that perfectly classified the adversarial data equally into both the legit and dga class."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49d920ad4db7657"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
