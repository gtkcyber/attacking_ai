{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SQL Injection Detection with Embeddings\n",
    "In this lab we will use embeddings to identify malicious SQL queries.  SQL injection is a technique where attackers inject malicious code into SQL queries for execution.  You can read more here: https://en.wikipedia.org/wiki/SQL_injection.\n",
    "You have already learned how to use lexical features to classify artifacts, but in this example, we will use a Random Forest Classifier, but instead of extracting features, we will use embeddings as a feature set.  You will then train a Random Forest Classifer to identify malicious queries.\n",
    "\n",
    "We will perform this experiment on two datasets, one was generated with ChatGPT and the other is from a Kaggle competition: (https://www.kaggle.com/datasets/sajid576/sql-injection-dataset)  The two files are:\n",
    "* `enriched_sql_injection_dataset_1000.csv`\n",
    "* `Modified_SQL_Dataset.csv`"
   ],
   "id": "9cdd1aa43a9fb549"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "DATA_PATH = '../data'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1:  Read in the Data\n",
    "The first step is to read in the sample dataset into a Pandas dataframe.  The first dataset we will use is the sample data contained in the file `enriched_sql_injection_dataset_1000.csv`."
   ],
   "id": "daf4693dabf7c3aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read in the data\n",
    "raw_data = pd.read_csv(f\"{DATA_PATH}/enriched_sql_injection_dataset_1000.csv\")\n",
    "raw_data.sample(5)"
   ],
   "id": "e43cdf7ff4a4f7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Load the Model and Generate Embeddings\n",
    "Now that we have the data in a Pandas Dataframe, the next step is to generate the embeddings. There are many models we could use for that, but for this exercise, we will use the model `paraphrase-MiniLM-L6-v2` (https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2) which is a sentence transformer model that maps sentences and paragraphs to a 384 dimensional dense vector space.\n",
    "\n",
    "This model provides a good balance between speed and effectiveness.\n",
    "\n",
    "You can use the `SentenceTransformer` module to create the model. Next, use the encode function to create the embeddings."
   ],
   "id": "313758f85b52998e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load embedding model\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ],
   "id": "c9f197c94dcdee7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generate embeddings\n",
    "raw_data['embedding'] = # Your code here...\n",
    "\n",
    "# Take a look at a sample of the data to make sure\n",
    "raw_data.sample(5)"
   ],
   "id": "31fbf8a95f2575aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert text labels to 0 and 1.\n",
    "raw_data['final_label'] = # Your code here...\n",
    "raw_data[['label', 'final_label']].sample(5)"
   ],
   "id": "f21ed0bef1d77e57",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that you generated the embeddings in a dataframe cell, you will need to extract them and convert them into a 2D list.  You can use the `tolist()` function to accomplish this.  You'll also need to extract the target vector.\n",
    "\n",
    "Once you've done that, split the data into training and testing sets. Use 25% of the data for testing.  Next train a `RandomForestClassifier` using the training data and make a list of `predictions` using the testing features."
   ],
   "id": "edf4b2c9f0d1e282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract arrays from dataframe\n",
    "features = #Your code here...\n",
    "target = #Your code here...\n",
    "\n",
    "# Split into train/test\n",
    "features_train, features_test, target_train, target_test = #Your code here...\n",
    "\n",
    "# Train the model\n"
   ],
   "id": "118db883e2bf7ca8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3:  Evaluate the Model's Performance\n",
    "Use Yellowbrick to create a confusion matrix and classification report to see how well the model performed."
   ],
   "id": "2f047a35d8fce703"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here...",
   "id": "5383c30d31ba9e9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Your code here...\n",
   "id": "d63339f0493f6072",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, this model performs very well with the defaults, however this was with a synthetic dataset.  One approach you might consider is to use a SQL tokenizer to tokenize the SQL queries before generating embeddings.\n",
    "\n",
    "```python\n",
    "sql_tokenizer = Tokenizer()\n",
    "sql_tokens = sql_tokenizer.tokenize(sql=\"SELECT * FROM users WHERE username = 'carol' AND password = '5845';\")\n",
    "token_types = [token.token_type.name for token in sql_tokens]\n",
    "```\n",
    "This converts the SQL query into the token string below:\n",
    "\n",
    "```\n",
    "['SELECT',\n",
    " 'STAR',\n",
    " 'FROM',\n",
    " 'VAR',\n",
    " 'WHERE',\n",
    " 'VAR',\n",
    " 'EQ',\n",
    " 'STRING',\n",
    " 'AND',\n",
    " 'VAR',\n",
    " 'EQ',\n",
    " 'STRING',\n",
    " 'SEMICOLON']\n",
    "```\n",
    "You could then generate embeddings from the token sequence.\n"
   ],
   "id": "d6334b9c33c11074"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 4:  Repeat with Real Data\n",
    "Next, we are going to train another model using a larger, non-generated dataset.  You can find the data in the file `Modified_SQL_Dataset.csv`.   How does this model perform?"
   ],
   "id": "9487b88c144ce786"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "raw_data2 = pd.read_csv(f\"{DATA_PATH}/Modified_SQL_Dataset.csv\")\n",
    "raw_data2['embedding'] = raw_data2['Query'].apply(lambda x: model.encode(x).tolist())\n",
    "\n",
    "features2 = np.array(raw_data2['embedding'].tolist())\n",
    "target2 = raw_data2['Label']\n",
    "\n",
    "# Split into train/test\n",
    "features2_train, features2_test, target2_train, target2_test = train_test_split(features2, target2, test_size=0.25)"
   ],
   "id": "83964c3ef30dcd83",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "clf2 = RandomForestClassifier()\n",
    "clf2.fit(features2_train, target2_train)\n",
    "predictions2 = clf2.predict(features2_test)"
   ],
   "id": "dc0bc4c3dba1329f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cm = ConfusionMatrix(clf2, classes=[0,1])\n",
    "cm.fit(features2_train, target2_train)\n",
    "cm.score(features2_test, target2_test)\n",
    "cm.show()"
   ],
   "id": "dbb6b8b9a847f01b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualizer = ClassificationReport(clf2, classes=[\"injection\", \"safe\"], support=True)\n",
    "visualizer.fit(features2_train, target2_train)\n",
    "visualizer.score(features2_test, target2_test)\n",
    "visualizer.show()"
   ],
   "id": "8f4712cda2070b59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Try It Yourself\n",
    "The code below is a wrapper function which allows a user to pass a SQL query to the model to see whether the model labels it as malicious or not.  Try it out and see how it does."
   ],
   "id": "2eb6142aed19bfaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_possible_malicious_query(query: str, classifier) -> bool:\n",
    "    df = pd.DataFrame(columns=['Query'])\n",
    "    df['Query'] = [query]\n",
    "    df['embedding'] = df['Query'].apply(lambda x: model.encode(query).tolist())\n",
    "    embedding = np.array(df['embedding'].tolist())\n",
    "    results = classifier.predict(embedding)\n",
    "    return results[0]\n"
   ],
   "id": "fae4bfa667aa7ff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(is_possible_malicious_query(\"SELECT * FROM users WHERE username = 'carol' AND password = '5845' OR 1=1;\", clf2))\n",
    "print(is_possible_malicious_query(\"SELECT * FROM users WHERE username = 'carol' AND password = '5845'\", clf2))"
   ],
   "id": "a150edfcb3d3385e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
